# Machine Learning & AI Project 1: FlapPyBird
## Danielle Li<br />Ocotber 21st, 2024
<div align="center">
  <img width="450" alt="nbaLogo" src="https://github.com/user-attachments/assets/d971ca97-5824-46e4-8bc2-ad0e05196c75">
</div>

### Literature Review/ Resources
Flappy Bird was a game I played often when I was younger and didn’t have wifi connections, so I thought it would be fun and interesting to do a game where the agent learns how to survive in a random environment (random pipes) instead of just optimizing a specific route. I, then, looked at different games like the Dinosaur game as well as this one and I thought it would be fun to play with Flappy Bird because it’s not a game that can just be found anymore. I found out a few years back that the original that I used to play was gone and there were only variations now, so I wanted to use this game during this project to train the bird. I found a lot of Youtube videos that already created an AI like [Code Bullet](https://www.youtube.com/watch?v=WSW-5m8lRMs) and [Tech With Tim](https://www.youtube.com/watch?v=MMxFDaIOHsE&list=PLzMcBGfZo4-lwGZWXz5Qgta_YNX3_vLS2&pp=iAQB) as well as a lot of github repos. I originally played around with [Mehmet Emin Eker’s Flappy Bird](https://github.com/mehmetemineker/flappy-bird.git) game which was the most accurate in terms of graphics, FPS, and sound effects. However, it was completely customized to be controlled by a player with a space bar and it had sounds programmed accordingly, so I decided to look at other models. I did some research that I will explain later here both in the literature review session and my implementation in my code, where I found that [NeuroEvolution of Augmenting Topologies](https://macwha.medium.com/evolving-ais-using-a-neat-algorithm-2d154c623828) or NEAT was the optimal way of training a Flappy Bird game, as that was very popular with the code I already found online. I then used Tech With Tim’s youtube tutorial basically to create the game but I adjusted a few elements within his NEAT to make a bird that would have more inputs in its neural networks, and I also changed the environment so that the pipes randomly moved up and down.

How Flappy Bird works in general is that basically you have your bird, and it doesn't move forward or backward. The bird only moves and down, and instead the pipes/the background is moving right to left which makes it look like the bird is moving; when in reality, the bird is moving up and down at the same x value. To set it up, I followed Tech With Tim’s video on how to create this project because I liked how he had used the NEAT to train his Flappy Bird game, but I wanted to adjust his NEAT in the end.

The AI algorithm I used to train the bird is [NeuroEvolution of Augmenting Topologies](https://nn.cs.utexas.edu/downloads/papers/stanley.cec02.pdf) or NEAT as explained by Kenneth O. Stanley and Risto Miikkulainen from The University of Texas at Austin where the architecture is evolving as NEAT evolves both the weights and structure or topology of the neural network. This allows the architecture to grow more complex over time. The genome represents a neural network and each genome is made up of two parts, a node which represents the neurons (input, hidden, and output nodes) and connections that connect nodes and have weights that will be optimized. NEAT starts with simple networks so just input and output nodes that are connected directly and then it slightly mutates the weights and the structure through the weighing and the addition of new nodes or connections. The algorithm combines parts of two parents' gnomes to create an “offspring” which allows the good features to be passed on and it's also called crossover. Then NEAT groups them into species based on similarity so that they can compete against one another to survive and those that outperform other species survive based on their fitness score. These species are then mutated and cross over to create new genomes and this is repeated until a complex enough structure is created to “survive” under all the conditions. 

I'm using my NEAT in a gym environment, which is a framework designed to simulate training environments for test reinforcement learning agents. It defines standardized interfaces for environments and uses reinforcement algorithms to train these ganents. I chose [NEAT specifically as my algorithm as my algorithms](https://ai.stackexchange.com/questions/10965/why-would-someone-use-neat-over-other-machine-learning-algorithms) because it’s more versatile and can be more customizable. Also, I realized that it would be more effective for my model to be trained using fitness as I wanted to select the best performing agents to be used to create the next generation of agents and mutating their neural networks. The bird needed to learn how to survive the longest, with its entire performance being based on a single score (fitness score) at the end of the game. I needed a more whole evaluation of the agent’s success/the highest score, which helps the model determine which gents to keep, discard, and use to evolve the next generation.

I also used ChatGPT to debug programs I ran into with installing libraries and formulas as well as just small bugs in general that were the result of human error (missing parentheses and typos).

### NEAT Implementation
#### Setup
Most of the data that was imported were the images needed to create the game. There was an image for the pipe (turned this upside down in the code to create the pipe below the bird), as well as the different motions that the bird went through (stationary, flapping wings, tilted up, and tilted down). There was also a setup that was needed for the window popup, but most of that information was provided by the tutorials by Tech With Tim as cited above. The rest of the setup was basic game controls like making the score, stating the generation, how many birds were alive, as well as making sure that the birds weren’t falling out of the window. Besides the general setup, I also modified the actual obstacles. Instead of the pipes being stationary and set at random heights, I made it so that the pipes were moving randomly up and down (random direction and speed) so that it would be more difficult to train the agent.

#### Neural Network
<div align="center">
  <img width="450" alt="imports" src="https://github.com/user-attachments/assets/29833d8c-85fc-48b1-a6e0-2ec69cc47b9c">
</div>

I have a neural network that's a sequential model made up of three dense layers. There is an input layer, a hidden layer, and an output layer.

For my input layer, it has 8 neurons, while the code I originally found only had 3 as it took the input of the vertical position of the bird and the vertical distance between the bird and the top/bottom of the closest pipe. For my model, the six neurons represented one of the inputs: Bird’s y position, Bird’s velocity, distance from the pipe, y position of the top of the pipe, y position of the bottom pipe, the distance from the bird to the ground, the pipe's speed, and the vertical direction of the pipe. Each of these inputs are then normalized, which means they are scaled to a range, to make sure that the network can learn effectively.

Then I have the hidden layer, which is where the neural network processes the inputs. In this hidden layer, because I’m using NEAT, the number of neurons changes as evolution progresses. You can explicitly set a number like one repository I read had set 8 neurons in their hidden layer. Instead, this algorithm will add or remove hidden neurons as it mutates and evolves networks. So basically NEAT will introduce new neurons based on the fitness improvements they bring to the birds' performance. Each of the neurons in this layer receives inputs from the neuron in the top or input layer. They apply a weighted sum followed by an activation function to introduce non-linearity, which lets the network learn complex patterns. The activation function determines whether a neuron activates based on a weighted pattern. Then each time we run a generation, the weighting is shifted to allow the data to have a better fit.

Then I have my output layer which has 2 neurons as it represents the decisions made by the neural network where one neuron represents the probability that the bird will flip while the other represents the probability that it won't flap. Before the values of each neuron become a probability, the output values are raw scores or logits, which comes from logistic regression, as it refers to the log-odds of a probability. These values are then processed through the softmax activation function which converts the logits to probabilities. The function transforms the raw outputs of the neural networks into a vector of probabilities using the equation below:

<div align="center">
  <img width="150" alt="imports" src=https://github.com/user-attachments/assets/2894b02c-78c5-4720-9292-48684a9f9a9a>
</div>

The softmax function takes a vector z of real numbers, which represents the outputs from the final layer of the neural network. Then, the inputs or elements in z are exponentiated using e to ensure that all values become positive. Finally, the exponentiated values are normalized after the exponentiated values are divided by the sum of all exponentiated values. This normalization step makes sure that the output values sum to 1, and since the output probabilities sum to 1, it lets us interpret the likelihood of each action.

After we take these actions, we can find the best bird’s best neural network,, which is the one that stayed alive the longest. This was determined through a fitness system like Darwin’s natural selection where I incorporated rewards into the way I calculated fitness. First, there’s an increase in fitness for each frame the bird remains alive. Each time it does its “fitness” increases by 0.1. Each type the bird successfully passes a pipe, the bird is rewarded +5. However, if the bird hits a pipe or hits the floor, the fitness decreases by 1. The birds, therefore, are encouraged to navigate through the pipes successfully. Also, if one of the N number of birds generated hits the ground or collides from a pipe, it is removed from the list of active birds, ending that bird’s game. The fitness score then reflects how well the bird performed based on survival time and number of pipes passed. So staying alive longer, navigating the pipes successfully, and avoiding collisions would earn the birds more points in the reward system, as only the bird with the highest points/fitness score is used in the next generation, ultimately rewarding the desirable behaviors that allows them to survive longer. The NEAT allows the birds to evolve over the generations as the birds learn which actions lead to higher rewards or fitness scores, the weighting in the hidden layer adapts, leading to improvement in their behavior and better performance in the game over time. Therefore, those with higher fitness scores have an increased chance of passing on their neural networks structure to the next generation, which is basically survival of the fittest. We also set a threshold for the fitness score so that once a bird reaches that score, we determined that the bird has mastered the game and the game can end to let us know what bird it was (which generation, node information, etc.).

#### Training Process
1. We create an initial population of 20 birds with the neural network’s hidden layer containing random weights
2. Then we simulate the population until all the birds have died
3. The next generation of N birds is created with the previous bird’s best neural networks.
4. Using the best performing bird’s neural networks, a small proportion of the weights is randomly mutated with random values in each of the new birds’ neural networks to create new behaviors
5. The previous steps are repeated until the birds can successfully navigate the environment

### Results
The results are printed using NEAT’s built-in progress reporting mechanism, called StdOutReporter. It prints information about each generation and fitness during the evolutionary process. ID is a “species” or a group of similar genomes and NEAT clusters similar neural networks in these species which maintains diversity in the population. Then we have the age of these species which is the generations. Then we have the number of genomes in this species, and we had set it to 20 in our configuration.txt it also tells us the best fitness score in this species, representing the best agent in this generation. The mechanism also tells us the adjusted fitness, which is what I had explained earlier as how the agents “mutate”, encouraging diversity. Then we have stag or stagnation, and since it’s 0 that means the species is improving but if the species doesn’t improve the value will increase and ENAT will eventually remove it from the population.

ALso, at the end, the program prints the bird that meets the fitness thrshold and then tels us which generation the bird was in. It also tells us the complexity of the agent, where the first node shows the number of hidden nodes in the neural network of that agent while the second number represents the number of connections. This includes connections from input nodes to hidden nodes and from hidden nodes to output nodes.

Since Flappy Bird is a simple game to train, it usually doesn’t take that many generations. For example, if I had 50 genomes in a species, it takes less than 10 generations for the program to learn how to play the game. This is if the frame rate is 60 frames per second (FPS). Obviously, as the speed is faster, the birds usually take more generations to train. Another thing we can do is set the number of genomes or individuals in our population. The more genomes there are, usually the faster the AI learns the game because the genome starts off with a much more diverse neural network than compared to the neural networks of just 10 genomes. We could also change how fast the pipes go by, the speed of the pipes moving vertically, and the FPS. For these factors, the higher the number was, the more generations it took to train the agent. 

I also plotted the highest fitness score of each generation, and I saw that even though the general trend would slowly increase, between 1-2 generations sometimes the fitness score would decrease. However, in the end, the final generation always had the highest fitness score.

<div align="center">
  <img width="400" alt="imports" src=https://github.com/user-attachments/assets/47437479-5a48-4794-9a58-3c60d3ada9e9>
</div>

### Problems
I originally thought that by adding more inputs like the velocity of the bird, the distance the bird was away from the pipes and the time since the bird last jumped it would cause the bird to train for less generations because it had more information to train from. However, I found that inputs such as the time since the last jump that didn’t have a significant impact on if the bird should jump next could be ignored because adding those factors lead my training to increase significantly. I had to play around with which inputs I wanted to include and exclude.

I also had to add rewards like for when the bird flew close to the pipe without hitting it because in the beginning the agents struggled with getting to the pipe. They were quite far away so I started adding incentives so that the birds would get closer and closer to navigating the pipe until it could pass through it and get the larger reward.

I also found that the number of generations that it took the model to learn sometimes varied drastically where some took less than 15 while others took almost 30-40 with some some needing more than 50 generations. I hadn’t changed anything between the two runs; I only pressed the run button. I often saw this inconsistency as I ran my program more and more; the number of generations it takes the bird to learn the game seems to heavily depend on the randomly generated neural networks in the beginning. This is something I want to touch on in my future work with this project.

### Future Work
In the future, I want to make it so that the number of generations it takes for the model to learn the game is more consistent because sometimes there is a drastic difference in the number of generations it takes to learn the game. Not only that but I also want to work on if there are other inputs that I could use to increase the efficiency of learning for NEAT, especially when I increase the speed, fps, or other factors to make the game harder. Another thing I could play with is to have different modes in flappy bird like horizontal/vertical flip or a change in the color gradient.
